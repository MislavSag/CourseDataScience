---
title: "Data Science"
subtitle: "Web scraping and web API's"
author: 
  - name: "Mislav Sagovac"
    affiliation: "Catholic University of Croatia"
    email: "mislav.sagovac@unicath.hr"
format: html
execute:
  echo: true
  eval: true
---

# Introduction

## Terminology

**Web scraping** is a technique used to extract information from websites. This tutorial demonstrates how to scrape web data in R using packages like `rvest` and `httr`. There is some difference between web crawling and web scraping. Web crawling is the process of automatically collecting information from the web, while web scraping is the process of extracting specific information from websites. but this distinctions is not always clear and not important for us right now.

**Web APIs** are a way to access data from websites and online services. They allow you to interact with websites programmatically, rather than through a web browser. When a website or online service provides an API, it means that they have made it possible for developers to access their data in a structured and controlled way.

## Web scraping vs. APIs

Why does web scraping exist if APIs are so powerful and do exactly the same work?

The main difference between web scraping and using APIs is that APIs are typically provided by the website or service to allow access to their data, while web scraping involves accessing data without the explicit permission of the website owner.

This means that using APIs is generally considered more ethical than web scraping, as it is done with the explicit permission of the website or service.

However, there are also some limitations to using APIs:

-   many APIs have rate limits, which means that they will only allow a certain number of requests to be made within a certain time period, i.e. you may not access large amounts of data;
-   not all websites or online services provide APIs, which means the only way to access their data is via web scraping.

## Ethical and legal considerations

It is important to note that web scraping can raise ethical concerns, as it involves accessing and using data from websites without the explicit permission of the website owner. It is a good practice to respect the terms of use for a website, and to seek written permission before scraping large amounts of data.

Just because you *can* scrape it, doesn't mean you *should*. A computer can process commands much, much faster than we can ever type them up manually. It's pretty easy to write up a function or program that can overwhelm a host server or application through the sheer weight of requests. Or, just as likely, the host server has built-in safeguards that will block you in case of a suspected malicious [attack](https://en.wikipedia.org/wiki/Denial-of-service_attack).

## Prerequisites

Before starting, ensure you have the following R packages installed:

``` r
# Install required packages
install.packages(c("httr", "rvest", "dplyr", "data.table"))
```

## Environment variables

Environment variables are useful for keeping sensitive information out of your scripts and code files, promoting better security and portability. These variables are often used to store:

-   API keys or tokens for accessing external services.
-   File paths or directory locations.
-   Configuration settings for R packages.

The `.Renviron` file is a plain text file where you can define environment variables. It is loaded when R starts, and any variables defined in it are available throughout the R session.

Benefits of .Renviron:

- Keeps sensitive information like API keys secure.
- Prevents hardcoding sensitive values into scripts.
- Enables consistent environment variable settings across sessions.

Here is example of `.Renviron` file:

```{.r}
# Define environment variables
API_KEY="your_api_key"
```

Once defined in .Renviron, you can access them in R using the Sys.getenv() function:

```{.r}
# Accessing the API_KEY environment variable
api_key <- Sys.getenv("API_KEY")
```

The last important thing is to add `.Renviron` file to your `.gitignore` file. This will prevent you from accidentally committing sensitive information to your repository.

# Web API's

## What is an API?

In nutshell, API is really just a collection of rules and methods that allow different software applications to interact and share information. This includes not only web servers and browsers, but also software packages like the R libraries

## Types of APIs

There are several types of APIs, including:

- **Web APIs**: These are APIs that are accessed over the internet using HTTP requests. They are commonly used to access data from online services, such as social media platforms, weather services, and financial services.
- **Library APIs**: These are APIs that are provided by software libraries to allow developers to access the functionality of the library in their own code.
- **Operating System APIs**: These are APIs that are provided by operating systems to allow developers to access the functionality of the operating system in their own code.

## How to use an API

To use an API, you need to know the following:

- The base URL of the API: This is the URL that you use to access the API.
- The endpoints of the API: These are the URLs that you use to access specific resources or data from the API.
- The methods of the API: These are the HTTP methods that you use to interact with the API, such as GET, POST, PUT, and DELETE.
- The parameters of the API: These are the parameters that you use to specify the data that you want to access from the API.

To use an API, you typically send an HTTP request to the API with the appropriate method, endpoint, and parameters, and the API will respond with the data that you requested.

## Examples of APIs

There are many APIs available that provide access to a wide range of data and services. Some examples of APIs include:

-   The Twitter API: This API allows you to access data from Twitter, such as tweets, users, and trends.
-   The OpenWeatherMap API: This API allows you to access weather data from around the world.
-   The Google Maps API: This API allows you to access maps and location data from Google Maps.

## API keys

Many APIs require you to use an API key to access the data. An API key is a unique identifier that is used to authenticate your requests to the API. You typically need to include your API key in the request to the API, either as a query parameter, a header, or in the request body.

To get an API key, you usually need to sign up for an account with the service that provides the API, and then generate an API key in your account settings.

## A bit more about API endpoints

A key point in all of this is that, in the case of web APIs, we can access information *directly* from the API database if we can specify the correct URL(s). These URLs are known as an **API endpoints**. 

API endpoints are in many ways similar to the normal website URLs that we're all used to visiting. For starters, you can navigate to them in your web browser. However, whereas normal websites display information in rich HTML content --- pictures, cat videos, nice formatting, etc. --- an API endpoint is much less visually appealing. Navigate your browser to an API endpoint and you'll just see a load of seemingly unformatted text. In truth, what you're really seeing is (probably) either [JSON](https://en.wikipedia.org/wiki/JSON) (**J**ava**S**cript **O**bject **No**tation) or [XML](https://en.wikipedia.org/wiki/XML) (E**x**tensible **M**arkup **L**anguage). 

You don't need to worry too much about the syntax of JSON and XML. The important thing is that the object in your browser --- that load of seemingly unformatted text --- is actually very precisely structured and formatted. Moreover, it contains valuable information that we can easily read into R (or Python, Julia, etc.) We just need to know the right API endpoint for the data that we want.

Let's practice doing this through a few example applications. I'll start with the simplest case (no API key required, explicit API endpoint) and then work through some more complicated examples. 

## Court Registry (Sudski registar) API

The docs for Court Registry API is available [here](https://sudreg-data.gov.hr/ords/r/srn_rep/vanjski-srn-rep/home). Here are [detaile desctiption](https://sudreg-data.gov.hr/ords/r/srn_rep/116/files/static/v11/Upute%20za%20razvojne%20in%C5%BEenjere%20-%20v3.0.0.pdf) for developers. Finally, [here](https://sudreg-data.gov.hr/api/javni/dokumentacija/open_api) you can see all available endpoints. These are all in Croatian, but you can easly translate it to English. Before you can use the API, you should register (enter name, e-mail and type of user) and get you API key.

![Court Registry API forma png](figures/sudski-registar-api-forma.png)

After you click on “Predaj zahtjev” the new page will show you the following:

![Court Registry API podaci png](figures/sudski-registar-api-1.png)

You can see the detail like `Client Id` and `Client Secret`. You will need thos to get the token and send request to Court Registry API.

Before, you can use the API, you should confirm your e-mail. E-mail looks like this

![Court Registry API email png](figures/sudski-registar-email.png)

To finally finish the registration, you should click on activate. Here is how page looks like:

![Court Registry activation png](figures/sudski-registar-email.png)

Your credentials are now activated and you can use the API. Here are API data redy to use:

![Court Registry API email png](figures/api-sudski-registar-podaci.png)

Let's see how we can use the API in R.

First. we sill need to import the package we sill use and get API token we will use later in our requests. You will have to use token url and some creditential data. It is best to save thin in `.Renviron` file as we explained in the introduction. 

Since the first part of URL is always the same when sending requests, we wil save it in URL varaible.

```{r}
library(httr)
library(data.table)

# Define 
token_url = "https://sudreg-data.gov.hr/api/oauth/token"
user_name = Sys.getenv("USER_SREG")
pass = Sys.getenv("PASS_SREG")
URL = "https://sudreg-data.gov.hr/api/javni/"

# Get Token following the documentation
token = POST(
  token_url,
  authenticate(user_name, pass),
  body = list(grant_type = "client_credentials"),
  encode = "form",
  httr::config(ssl_verifypeer = FALSE)
)
token = content(token)
print(token)
```

Now, let's finally get some data. List of endpoints is available [here](https://sudreg-data.gov.hr/api/javni/dokumentacija/open_api). 

```{r}
res = GET(paste0(URL, "subjekti"),
          add_headers(
            "Authorization" = paste0("Bearer ", token$access_token),
            "Content-Type" = "application/json"
          ),
          query = list(limit = 100, only_active = FALSE)
          )
res = content(res)

print(head(res, 2))
```

Lets explain above code:

-   `GET` function is used to send a GET request to the API endpoint. The first argument is the URL of the endpoint we want to access. 
- The `add_headers` function is used to add the Authorization header to the request, which includes the access token we received when we authenticated with the API. 
- The `Content-Type` header specifies that the content type of the request is JSON. 
- The `query` argument is used to specify the query parameters of the request, which in this case include the limit and only_active parameters.
- `content` function is used to extract the content of the response, which is the data that the API returns.

Now, we would like to clean the data. We want to have data.frame or data.table, not the lists. Here is how can we do tha very simepl using data.table package.

```{r}
# Convert all list elements to data.table
res_clean = lapply(res, as.data.table)

# Rbind all elements
res_clean = rbindlist(res_clean, fill = TRUE)

head(res_clean)
```
We can write a function to make above steps easier. Here is how we can do that:

```{r}
# Wrap for GET request to court registry
get_sreg_url = function(url) {
  res = GET(url,
            add_headers("Authorization" = paste0("Bearer ", token$access_token),
                        "Content-Type" = "application/json"))
  return(content(res))
}

# Wrap for GET request to court registry
get_sreg = function(tag, q = list(limit = 100, only_active = TRUE), clean = TRUE) {
  url = modify_url(
    paste0(URL, tag),
    query = q
  )
  # Make GET request
  cont = get_sreg_url(url)
  # Clean if needed
  if (clean == TRUE) {
    return(rbindlist(cont, fill = TRUE))
  } else {
    return(cont)
  }
}
```

Now, we can use this function to get data from Court Registry API. Here is how we can do that:

```{r}
# Get data from court public registry
subjects = get_sreg("subjekti")

head(subjects)
```

Let's try some other endpoints

```{r}
# Headquarters
headq = get_sreg(tag = "sjedista")
head(headq)
```

```{r}
# Core business
cores = get_sreg(tag = "pretezite_djelatnosti")
head(cores)
```

```{r}
# Industry
industry = get_sreg(tag = "predmeti_poslovanja")
head(industry)
```

```{r}
# Evidence activities
activities = get_sreg(tag = "evidencijske_djelatnosti")
head(activities)
```

```{r}
# Short name
short_names = get_sreg(tag = "evidencijske_djelatnosti")
head(short_names)
```

There can be a problem if output to API calls have many rows. One one side, API can restrict number of rows in output, on the other side, we can have a problem with memory. Or it can take long time and we will get timeout error.

We can solve this by using a loop. Here is how we can do that/:

```{r}
# Get data from court public registry in a loop
get_sreg_loop = function(tag = "subjekti", 
                         by = 100, 
                         total = 1000) {
  # Define offsets
  offset_seq = seq(0, total, by = by)
  offset_seq = format(offset_seq, scientific = FALSE)
  offset_seq = gsub("\\s+", "", offset_seq)
  # Define urls
  urls = lapply(offset_seq, function(x) {
    modify_url(
      paste0(URL, tag),
      query = list(
        offset = x,
        limit = format(by, scientific = FALSE),
        only_active = FALSE
      )
    )
  })
  res_l = lapply(urls, get_sreg_url)
  res_l = res_l[!sapply(res_l, function(x) length(x) == 0)]
  res_l = lapply(res_l, function(l) {
    rbindlist(lapply(l, as.data.table), fill = TRUE)
  })
  rbindlist(res_l, fill = TRUE)
}

# Business subjects
subjects_with_loop = get_sreg_loop("subjekti")
head(subjects_with_loop)
```

The resulted `subjects_with_loop` object have `{r} nrow(subjects_with_loop)` nrows as we would expect.


# Web scraping

## Web pages

Almost anyone is familiar with web pages (otherwise you would not be here), but what if we tell you that how you see a site is different from how Google or your browser does?

In fact, when you type any site address in your browser, your browser will download and render the page for you, but for rendering the page it needs some instructions.

There are 3 types of instructions:

HTML: describes a web page’s infrastructure; CSS: defines the appearance of a site; JavaScript: decides the behavior of the page. Web scraping is the art of extracting information from the HTML, CSS and Javascript lines of code. The term usually refers to an automated process, which is less error-prone and faster than gathering data by hand.

I want to forewarn you that webscraping typically involves a fair bit of detective work. You will often have to adjust your steps according to the type of data you want, and the steps that worked on one website may not work on another. (Or even work on the same website a few months later). All this is to say that *webscraping involves as much art as it does science*.

The good news is that both server-side and client-side websites allow for webscraping.[^1] If you can see it in your browser, you can scrape it.

[^1]: As we'll see during the next lecture, scraping a website or application that is built on a client-side (i.e. API) framework is often easier; particularly when it comes to downloading information *en masse*.

## HTML and CSS

Before starting it is important to have a basic knowledge of HTML and CSS. This section aims to briefly explain how HTML and CSS work

Starting from HTML, an HTML file looks like the following piece of code.

``` html
<!DOCTYPE html>
<html lang="en">
<body>

<h1 href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss"> Carl Friedrich Gauss</h1>
<h2> Biography </h2>
<p> Johann Carl Friedrich Gauss was born on 30 April 1777 in Brunswick. </p>
<h2> Profession </h2>
<p> Gauss is considered as one of the greatest mathematician, statistician and physicist of all time. </p>

</body>
</html>
Those instructions produce the following:
```

As you read above, HTML is used to describe the infrastructure of a web page, for example we may want to define the headings, the paragraphs, etc.

This infrastructure is represented by what are called tags (for example

```{.html}
<h1>

...\<\h1\>
```

or

```{.html}
<p>

...\<\p\>
```

are tags). Tags are the core of an HTML document as they represent the nature of what is inside the tag (for example h1 stands for heading 1). It is important to observe that there are two types of tags:

starting tags (e.g.

`<h1>`

) ending tags (e.g. `\<\h1\>`) This is what allows to nest different tags.

Tags can also have attributes, for example in \<h1 href="https://en.wikipedia.org/wiki/Carl_Friedrich_Gauss"Carl Friedrich Gauss

</h1>

, href is an attribute of the tag h1 that specifies an URL.

As the output of the above HTML code is not super elegant, CSS is used to style the final website. For example CSS is used to define the font, the color, the size, the spacing and many more features of a website.

What is important for this article are CSS selectors, which are patterns used to select elements. The most important is the .class selector, which selects all elements with the same class. For example the .xyz selector selects all elements with class="xyz".

## Usefull R packages for web scraping

There are several packages for web scraping in R, every package has its strengths and limitations. Here is the list of most popular ones

-   rvest: rvest is a part of the tidyverse and is designed to make scraping information from the web easier. It is a great package for beginners as it is easy to use and has a lot of documentation. We will use rvest in our examples.
-   httr: httr is a package that allows you to send HTTP requests and work with web APIs. It is a more general package than rvest and can be used for a wide range of web-related tasks.
-   RSelenium: RSelenium is a package that allows you to control a web browser from R. This can be useful for scraping websites.
-   chromium: chromium is a package that allows you to run a headless version of the Chromium web browser from R. This can be useful for scraping websites that require JavaScript to render the page.
-   jsonlite: jsonlite is a package for working with JSON data. It can be useful for parsing JSON responses from web APIs.
-  xml2: xml2 is a package for working with XML and HTML documents. It can be useful for parsing web pages and extracting information.

## Web scrapping process

The web scraping process can be broken down into the following steps:

1. Send GET or POST request to the website you want to scrape.
2. Parse the HTML content of the website.
3. Extract the information you want from the HTML content (getting HTML attributes).

## Simple example

Let's start with very simple example. We will replicate 3 steps above by sending request and scraping content of NT times website. We will use rvest package for this.

Sending the request to the page is simple, `rvest` provides the `read_html` function, which returns an object of `html_document` type:

```{r}
library(rvest)

# Send GET request to the website
url = "https://www.nytimes.com/"
page = read_html(url)

# Print the page
print(page)
```
Next step is parsing html content. As we saw in the last chunk of code, NYT_page contains the raw HTML code, which is not so easily readable.

In order to make it readable from R it has to be parsed, which means generating a Document Object Model (DOM) from the raw HTML. DOM is what connects scripts and web pages by representing the structure of a document in memory. If you retrieve the HTTP request using Node.js, you can give the raw HTML response to R for parsing and further analysis.

`rvest` provides 2 ways to select HTML elements:

-   CSS selectors with following syntax

```{r}
# Select elements using CSS selectors
elements = html_nodes(page, "h2")

# Print the elements
print(elements)
```

-   XPath with following syntax:

```{r}
# Select elements using CSS selectors
elements = html_nodes(page, "h2")

# Print the elements
print(elements)
```
CSS selectors are more common and easier to use, so we will use them in rest of the notebook. CSS selectors are patterns used to select elements in an HTML document. They are ussually used to apply styles to elements, but they can also be used to select elements for scraping.

The easiest way to obtain a CSS selector is opening the inspect mode, find the element you desire and right click on it. Then click on copy and copy selector.

Alternatively, you can use external softwer for CSS selector generation. For example, you can use [SelectorGadget](https://selectorgadget.com/) in chrome. On Edge, you can use [CSS selector](https://microsoftedge.microsoft.com/addons/detail/css-selector/nloicnlfkohbekabfhgdfgmkdgghfhnh) extension.

We can show one more example where we select element with css selector:

```{r}
# CSS selector
page %>% 
  html_elements(".css-1gg6cw2")  %>% 
  html_text() %>% 
  head()

# XPath
xpath_ = '//*[contains(concat( " ", @class, " " ), concat( " ", "css-1gg6cw2", " " ))]'
page %>% 
  html_elements(xpath = xpath_) %>%
  html_text()
```

## Example 2 - Wikipedia

Let's try to scrape some data from Wikipedia. We will try to get the list of all countries in the world. We will use the following [URL](https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_and_their_capitals_in_native_languages). We will use rvest again.

```{r}
# Send GET request to the website
url = "https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_and_their_capitals_in_native_languages"
page = read_html(url)

# Select the table with countries
countries_table = html_elements(page, "table")[[1]]

# Extract the table
countries = html_table(countries_table)

# Print the table
print(head(countries))
```

Another example is to scrape data from Formula 1 Wikipedia’s voice and create a CSV file containing the name, the nationality, the number of podiums and some other statistics for every pilot.

The table we are going to scrape is avaialble [here]|(https://en.wikipedia.org/wiki/List_of_Formula_One_drivers). We will use the following code to scrape the data:

Again we repeat what we did before with the NYT example:

```{r}
# Send GET request to the website
url = "https://en.wikipedia.org/wiki/List_of_Formula_One_drivers"
page = read_html(url)
```

Searching in the HTML code we find that the table is a table element with the sortable attribute:

```{r}
drivers_F1 = html_element(page, ".wikitable.sortable") %>%
  html_table()
```

In the chunk of code above, the html_table function is used to render the HTML code into tables.

To inspect it, we display the first and last observations, and the structure of the dataset:

```{r}
head(drivers_F1)
```

## Example 3 - Get text from vecernji

```{r}
url = "https://www.vecernji.hr/vijesti/snijeg-obilne-padaline-snazni-vjetrovi-stize-velika-zimska-oluja-swe-objavio-koje-ce-zemlje-biti-na-udaru-1821372"
read_html(url) %>% 
  html_elements(".single-article__content") %>% 
  html_children() %>% 
  html_elements("p") %>% 
  html_text()


```

```{r}
read_html("https://www.vecernji.hr/") %>% 
  html_elements("a") %>%
  html_attr("href")
```



